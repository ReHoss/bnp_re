{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationnary clustering and companies evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we build stationnary clusters on a given period (from 2010 to 2018). They are meant to be representative of the financial segmentation of the market during the period. The period chosen exclude the 2008 economic crisis since we first want to base our study on a stationnary global economic regime (post-crisis economic growth rebound).\n",
    "\n",
    "After building clusters representing different long-term (8 years) performances of the companies available on the 2010-2018 period, we observe companies affilliation to clusters with respect to time (year by year). We hope observing pattern such as companies switching from one cluster to another and we would like to give the reasons of those transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T15:08:35.896360Z",
     "start_time": "2020-06-16T15:08:35.512902Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (12.0, 8.0)\n",
    "\n",
    "\n",
    "path_data = '/home/rho/Documents/work/bnp_re/company_clustering/data/'\n",
    "path = path_data + 'df_company_quant.pickle'\n",
    "path_qualitative = path_data + 'df_company_qualitative.pickle'\n",
    "path_adress = path_data + 'df_company_adress.pickle'\n",
    "\n",
    "\n",
    "df_company_quant = pd.read_pickle(path)\n",
    "df_company_qualitative = pd.read_pickle(path_qualitative)\n",
    "df_company_adress = pd.read_pickle(path_adress)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the analysis we have to highlight yearly financial data for all companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape the data to add a year index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T15:08:36.800726Z",
     "start_time": "2020-06-16T15:08:35.898320Z"
    }
   },
   "outputs": [],
   "source": [
    "df = (df_company_quant\n",
    " .drop(columns=['LAST_STATUS_DATE', 'LAST_FINANCIAL_DATE', 'NB_SHAREHOLDERS',\n",
    "                'NB_SHAREHOLDINGS', 'ENTITY_START_DATE', ])\n",
    " .replace(0, np.nan))\n",
    "\n",
    "df = (df\n",
    " .T\n",
    " .set_index([df.T.index.str.split('_').str[-1].set_names('year'),\n",
    "             df.T.index.str.split('_').str[:-1].set_names('variable')\n",
    "                                                     .map(lambda x: f'{x[0]}_{x[1]}'\n",
    "                                                                 if len(x) == 2\n",
    "                                                                    else x[0])])\n",
    " .unstack('variable')\n",
    " .stack(0)\n",
    " .assign(EBITDA_BY_INCOME=lambda df: df.EBITDA / df.OP_INCOME,\n",
    "         ASSET_BY_INCOME=lambda df: df.ASSET / df.OP_INCOME,\n",
    "         RESULT_BY_INCOME=lambda df: df.RESULT / df.OP_INCOME,\n",
    "         EBITDA_BY_STAFF=lambda df: df.EBITDA / df.STAFF_COST,\n",
    "         INCOME_BY_STAFF=lambda df: df.OP_INCOME / df.STAFF_COST,\n",
    "         EMPLOYEE_COST=lambda df: df.NB_EMPLOYEES / df.STAFF_COST,\n",
    "         )\n",
    " \n",
    " .drop('2019')\n",
    " .drop('NB_EMPLOYEES', axis=1))\n",
    "\n",
    "df.index = df.index.set_names(['year', 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing returns by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T15:08:37.179040Z",
     "start_time": "2020-06-16T15:08:36.803137Z"
    }
   },
   "outputs": [],
   "source": [
    "df_returns = (df             \n",
    " .unstack(level=1)\n",
    " .pct_change()\n",
    " .replace(0, np.nan))\n",
    "\n",
    "df_returns = df_returns.multiply(df.unstack(level=1).shift().apply(np.sign))\n",
    "df_returns.columns = df_returns.columns.set_names(['variable', 'id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building DataFrame contaning both returns and absolute values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As requested by Xavier, we mix returns and absolute values to compute clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all =\\\n",
    "(df_returns\n",
    " .stack(1)\n",
    " .join(df[['EBITDA', 'OP_INCOME', 'STAFF_COST']], lsuffix='_RETURNS')\n",
    " .unstack(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below a sample of the DataFrame used to compute clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_all\n",
    " .stack(1)\n",
    " .sample(frac=1)\n",
    " .head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregating values over the whole period to generate stationnary companies (time independant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take the median over the entire time period in order to obtain for each of the variables, a value that represents the global performance of the company over the whole economic period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T15:08:40.237790Z",
     "start_time": "2020-06-16T15:08:37.216391Z"
    }
   },
   "outputs": [],
   "source": [
    "df_final = (df_all\n",
    " .stack(level=1)\n",
    " .groupby(level=1)\n",
    " .median()\n",
    "#  # To drop before\n",
    " .join(df_company_quant[['NB_SHAREHOLDERS', 'NB_SHAREHOLDINGS', 'ENTITY_START_DATE', 'NB_EMPLOYEES_2018']])\n",
    " .assign(AGE=lambda df: (2020 - df.ENTITY_START_DATE.dt.year))\n",
    " .rename(columns={'NB_EMPLOYEES_2018': 'NB_EMPLOYEES'})\n",
    " .drop('ENTITY_START_DATE', axis=1)\n",
    " .dropna()\n",
    ")\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute stationnary clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T15:08:40.237790Z",
     "start_time": "2020-06-16T15:08:37.216391Z"
    }
   },
   "outputs": [],
   "source": [
    "alpha = 0.01\n",
    "\n",
    "mask_outliers = (df_final\n",
    "     .apply(lambda x:\n",
    "            ((x > x.quantile(1 - alpha))\n",
    "           | (x < x.quantile(alpha))))\n",
    "     .any(axis=1))\n",
    "\n",
    "from sklearn import mixture\n",
    "\n",
    "gmm = mixture.GaussianMixture(n_components=4)\n",
    "gmm.fit(df_final.loc[~mask_outliers])\n",
    "\n",
    "df = (df_final\n",
    " .assign(cluster=gmm.predict(df_final))\n",
    " .loc[~mask_outliers])\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "ncol = 4\n",
    "nrow = len(df_final.columns) // ncol + 1\n",
    "variable_names = df.drop(columns='cluster').columns\n",
    "nb_variables = len(variable_names)\n",
    "colors = ['red', 'blue', 'orange', 'green']\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=nrow, cols=ncol, shared_yaxes=False, subplot_titles=variable_names,\n",
    "                    vertical_spacing=0.05, horizontal_spacing=0.05)\n",
    "\n",
    "for i, column_name in enumerate(variable_names):\n",
    "    for j in range(df.cluster.max() + 1):\n",
    "\n",
    "        col = (i % ncol) + 1\n",
    "        row = i // (ncol) + 1\n",
    "        fig.add_trace(go.Box(y=df.loc[df.cluster.eq(j), column_name],\n",
    "                             marker_color=colors[j],\n",
    "                             name=j),\n",
    "                      row=row,\n",
    "                      col=col)\n",
    "\n",
    "fig.update_layout(showlegend=False,\n",
    "                  autosize=False,\n",
    "                  width=1000,\n",
    "                  height=1000)\n",
    "\n",
    "fig.update_layout()\n",
    "fig.show()\n",
    "del fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T15:08:40.237790Z",
     "start_time": "2020-06-16T15:08:37.216391Z"
    }
   },
   "outputs": [],
   "source": [
    "df_final = (df_all\n",
    " .stack(level=1)\n",
    " .groupby(level=1)\n",
    " .median()\n",
    "#  # To drop before\n",
    " .join(df_company_quant[['NB_SHAREHOLDERS', 'NB_SHAREHOLDINGS', 'ENTITY_START_DATE', 'NB_EMPLOYEES_2018']])\n",
    " .drop(columns=['NB_SHAREHOLDINGS', 'NB_SHAREHOLDERS'])\n",
    " .assign(AGE=lambda df: (2020 - df.ENTITY_START_DATE.dt.year))\n",
    " .rename(columns={'NB_EMPLOYEES_2018': 'NB_EMPLOYEES'})\n",
    " .drop('ENTITY_START_DATE', axis=1)\n",
    " .dropna())\n",
    "\n",
    "alpha = 0.01\n",
    "\n",
    "mask_outliers = (df_final\n",
    "     .apply(lambda x:\n",
    "            ((x > x.quantile(1 - alpha))\n",
    "           | (x < x.quantile(alpha))))\n",
    "     .any(axis=1))\n",
    "\n",
    "from sklearn import mixture\n",
    "\n",
    "gmm = mixture.GaussianMixture(n_components=4, random_state=94)\n",
    "gmm.fit(df_final.loc[~mask_outliers])\n",
    "\n",
    "df = (df_final\n",
    " .assign(cluster=gmm.predict(df_final))\n",
    " .loc[~mask_outliers])\n",
    "\n",
    "df.to_pickle(path=path_data + 'df_clustering.pickle', protocol=0)\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "ncol = 4\n",
    "nrow = len(df_final.columns) // ncol + 1\n",
    "variable_names = df.drop(columns='cluster').columns\n",
    "nb_variables = len(variable_names)\n",
    "colors = ['red', 'blue', 'orange', 'green']\n",
    "\n",
    "\n",
    "fig = make_subplots(rows=nrow, cols=ncol, shared_yaxes=False, subplot_titles=variable_names,\n",
    "                   vertical_spacing=0.05, horizontal_spacing=0.05)\n",
    "\n",
    "for i, column_name in enumerate(variable_names):\n",
    "    for j in range(df.cluster.max() + 1):\n",
    "\n",
    "        col = (i % ncol) + 1\n",
    "        row = i // (ncol) + 1\n",
    "        fig.add_trace(go.Box(y=df.loc[df.cluster.eq(j), column_name],\n",
    "                             marker_color=colors[j],\n",
    "                             name=j),\n",
    "                      row=row,\n",
    "                      col=col,)\n",
    "\n",
    "fig.update_layout(showlegend=False,\n",
    "                  autosize=False,\n",
    "                  width=1000,\n",
    "                  height=1000)\n",
    "\n",
    "fig.update_layout()\n",
    "fig.show()\n",
    "del fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cluster 2 (yellow) contains companies that have the best growth over the period since it has the highest median in almost all financial variables (ASSET, EBITDA, OP_INCOME, PROFIT_RATIO, ...). However it has also a growing STAFF_COST.\n",
    "\n",
    "\n",
    "- Cluster 0 and 1 (red and blue) are quite similar, they might represent company with medium performances. Red has a slightly better OP_INCOME and ASSET returns over the 2010-2018 period. And is better to reduce EMPLOYEE_COST. But its STAFF_COST is more increasing than blue cluster. Finally, cluster 0 contains a small number of employees while blue contains a way higher number of employees. Blue is older than red.\n",
    "\n",
    "\n",
    "- Green represents small companies with bad results.\n",
    "\n",
    "\n",
    "- Red are small companies that have a quite good performance during the period while green are small companies without great results over the 2010-2018 horizon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign cluster names\n",
    "\n",
    "Defining name of clusters here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_names = ['Young - Slow growth', 'Old - Big companies', 'Old - Slow growth', 'Young - Fast growth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_final\n",
    " .assign(cluster=gmm.predict(df_final),\n",
    "         cluster_count=lambda df:  df.groupby('cluster').transform('count').iloc[:, 0])\n",
    " .replace({'cluster': {i: cluster_names[i] for i in range(len(cluster_names))}})\n",
    " .to_pickle(path=path_data + 'df_clustering_graphs.pickle', protocol=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of companies by cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T15:10:40.643438Z",
     "start_time": "2020-06-16T15:10:40.383917Z"
    }
   },
   "outputs": [],
   "source": [
    "(df_final\n",
    " .assign(cluster=gmm.predict(df_final))\n",
    " .cluster\n",
    " .value_counts()\n",
    " .plot\n",
    " .bar()\n",
    " .set(xlabel='Cluster Id',\n",
    "      ylabel='Number of companies'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "- Cluster 3 contains the most companies.\n",
    "- Cluster 0 is the second most populated cluster.\n",
    "- Both clusters 2 and 1 have the same amount of companies.\n",
    "- Clustering is quite well balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-16T15:10:40.643438Z",
     "start_time": "2020-06-16T15:10:40.383917Z"
    }
   },
   "outputs": [],
   "source": [
    "(df_final\n",
    " .assign(cluster=gmm.predict(df_final))\n",
    " .cluster\n",
    " .value_counts()\n",
    " .divide(len(df_final.index))\n",
    " .plot\n",
    " .bar()\n",
    " .set(xlabel='Cluster Id',\n",
    "      ylabel='Rate of companies'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "- Cluster 3 contains the most companies.\n",
    "- Cluster 0 is the second most populated cluster.\n",
    "- Both clusters 2 and 1 have the same amount of companies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity type with respect to clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(df_final\n",
    " .assign(cluster=gmm.predict(df_final))\n",
    " \n",
    " .join(df_company_qualitative)\n",
    " .groupby(['cluster', 'FIRST_ACTIVITY_TYPE_ID'])\n",
    " .count()\n",
    " .ASSET\n",
    " .unstack(0)\n",
    " .dropna()\n",
    " .loc[lambda x: x.min(axis=1) > 40]\n",
    " .plot\n",
    " .bar(figsize=(16,9))\n",
    " .set(xlabel='Entity type',\n",
    "      ylabel='Number of companies')\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 5510 Hotels and similar accommodation\n",
    "- 4120 Construction of residential and non-residential buildings\n",
    "- 7022 Business and other management consultancy activities\n",
    "- 4941 Freight transport by road\n",
    "- 4321 Electrical installation\n",
    "- 7112 Engineering activities and related technical consultancy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "- 5510 represents activity of cluster 3.\n",
    "- Clusters 1 and 0 represent activity 4120 mainly.\n",
    "- Cluster 0 represents 4321 and 4941."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity type with respect to clusters (rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test\n",
    "(df_final\n",
    " .assign(cluster=gmm.predict(df_final),\n",
    "         cluster_count=lambda df:  df.groupby('cluster').transform('count').iloc[:, 0])\n",
    " \n",
    " .join(df_company_qualitative)\n",
    " .groupby(['cluster', 'FIRST_ACTIVITY_TYPE_ID', 'cluster_count'], as_index=False)\n",
    " .count()\n",
    " .dropna()\n",
    " \n",
    " .assign(rate=lambda df: df.ASSET.div(df.cluster_count))\n",
    " \n",
    " .set_index(['cluster', 'FIRST_ACTIVITY_TYPE_ID'])\n",
    " \n",
    " .rate\n",
    " \n",
    " .unstack(0)\n",
    " .dropna()\n",
    " .loc[lambda x: x.min(axis=1) > 0.003]\n",
    " .plot\n",
    " .bar(figsize=(16,9))\n",
    " .set(xlabel='Entity type',\n",
    "      ylabel='Number of companies')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entity type with respect to clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(df_final\n",
    " .assign(cluster=gmm.predict(df_final))\n",
    " \n",
    " .join(df_company_qualitative)\n",
    " .groupby(['cluster', 'ENTITY_DETAILED_TYPE_LABEL'])\n",
    " .count()\n",
    " .ASSET\n",
    " .unstack(0)\n",
    " .dropna()\n",
    " .loc[lambda x: x.min(axis=1) > 40]\n",
    " .plot\n",
    " .bar(figsize=(16,9))\n",
    " .set(xlabel='Entity type',\n",
    "      ylabel='Number of companies'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "- Cluster 3 contains the most of Sole Corporations and Joint-Stock companies.\n",
    "- Cluster 0 contains the most LLC entities.\n",
    "- Clusters are quite well balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "(df_final\n",
    " .assign(cluster=gmm.predict(df_final))\n",
    " \n",
    " .join(df_company_adress)\n",
    " .groupby(['cluster', 'ENTITY_CITY'])\n",
    " .count()\n",
    " .ASSET\n",
    " .unstack(0)\n",
    " .dropna()\n",
    " .loc[lambda x: x.min(axis=1) > 40]\n",
    " .plot\n",
    " .bar(figsize=(16,9))\n",
    " .set(xlabel='Entity type',\n",
    "      ylabel='Number of companies'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "- Most of the companies come from Madrid and Barcelona.\n",
    "- Clusters are not discriminated by city."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_all.stack(1)\n",
    "df.index = df.index.set_names(['year', 'id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some values\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .join(df_company_quant[['ENTITY_START_DATE', 'NB_EMPLOYEES_2018']],\n",
    "       on='id')\n",
    " .assign(AGE=lambda df: (2020 - df.ENTITY_START_DATE.dt.year))\n",
    " .drop(columns='ENTITY_START_DATE')\n",
    " .rename(columns={'NB_EMPLOYEES_2018': 'NB_EMPLOYEES'})\n",
    " .dropna()\n",
    " .assign(cluster=lambda df: gmm.predict(df))\n",
    " .cluster\n",
    " .unstack(0)\n",
    " .dropna()\n",
    " .head(25)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickelize DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .join(df_company_quant[['ENTITY_START_DATE', 'NB_EMPLOYEES_2018']],\n",
    "       on='id')\n",
    " .assign(AGE=lambda df: (2020 - df.ENTITY_START_DATE.dt.year))\n",
    " .drop(columns='ENTITY_START_DATE')\n",
    " .rename(columns={'NB_EMPLOYEES_2018': 'NB_EMPLOYEES'})\n",
    " .dropna()\n",
    " .assign(cluster=lambda df: gmm.predict(df))\n",
    " .replace({'cluster': {i: cluster_names[i] for i in range(len(cluster_names))}})\n",
    " .to_pickle(path=path_data + 'df_transitions.pickle', protocol=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transitions statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .join(df_company_quant[['ENTITY_START_DATE', 'NB_EMPLOYEES_2018']],\n",
    "       on='id')\n",
    " .assign(AGE=lambda df: (2020 - df.ENTITY_START_DATE.dt.year))\n",
    " .drop(columns='ENTITY_START_DATE')\n",
    " .rename(columns={'NB_EMPLOYEES_2018': 'NB_EMPLOYEES'})\n",
    " .dropna()\n",
    " .assign(cluster=lambda df: gmm.predict(df))\n",
    " .cluster\n",
    " .unstack(0)\n",
    " .dropna()\n",
    " .assign(transition_crisis=lambda df: df['2006'] == df['2007'])\n",
    " .transition_crisis\n",
    " .value_counts()\n",
    " .plot\n",
    " .bar()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .join(df_company_quant[['ENTITY_START_DATE', 'NB_EMPLOYEES_2018']],\n",
    "       on='id')\n",
    " .assign(AGE=lambda df: (2020 - df.ENTITY_START_DATE.dt.year))\n",
    " .drop(columns='ENTITY_START_DATE')\n",
    " .rename(columns={'NB_EMPLOYEES_2018': 'NB_EMPLOYEES'})\n",
    " .dropna()\n",
    " .assign(cluster=lambda df: gmm.predict(df))\n",
    " .cluster\n",
    " .unstack(0)\n",
    " .dropna()\n",
    " .loc[:, ['2009', '2010']]\n",
    " .astype(int)\n",
    " .apply(tuple, axis=1)\n",
    " .value_counts()\n",
    " .plot\n",
    " .bar(figsize=(16, 9))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = ['2007', '2008']\n",
    "\n",
    "(df\n",
    " .join(df_company_quant[['ENTITY_START_DATE', 'NB_EMPLOYEES_2018']],\n",
    "       on='id')\n",
    " .assign(AGE=lambda df: (2020 - df.ENTITY_START_DATE.dt.year))\n",
    " .drop(columns='ENTITY_START_DATE')\n",
    " .rename(columns={'NB_EMPLOYEES_2018': 'NB_EMPLOYEES'})\n",
    " .dropna()\n",
    " .assign(cluster=lambda df: gmm.predict(df))\n",
    " .cluster\n",
    " .unstack(0)\n",
    " .dropna()\n",
    " .loc[:, period]\n",
    " .astype(int)\n",
    " .groupby(period)\n",
    " .size()\n",
    " .unstack(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "period = ['2007', '2008']\n",
    "\n",
    "data = (df\n",
    " .join(df_company_quant[['ENTITY_START_DATE', 'NB_EMPLOYEES_2018']],\n",
    "       on='id')\n",
    " .assign(AGE=lambda df: (2020 - df.ENTITY_START_DATE.dt.year))\n",
    " .drop(columns='ENTITY_START_DATE')\n",
    " .rename(columns={'NB_EMPLOYEES_2018': 'NB_EMPLOYEES'})\n",
    " .dropna()\n",
    " .assign(cluster=lambda df: gmm.predict(df))\n",
    " .cluster\n",
    " .unstack(0)\n",
    " .dropna()\n",
    " .loc[:, period]\n",
    " .astype(int)\n",
    " .groupby(period)\n",
    " .size()\n",
    " .unstack(0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# data=[[1, 25, 30, 50, 1], [20, 1, 60, 80, 30], [30, 60, 1, 5, 20]]\n",
    "fig = px.imshow(data,\n",
    "                labels=dict(x=\"New Cluster\", y=\"Initial Cluster\", color=\"Number of companies\"),\n",
    "                x=data.columns,\n",
    "                y=data.columns,\n",
    "                title=\"Clusters transition from 2007 to 2008\")\n",
    "\n",
    "fig.update_xaxes(side=\"top\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remarks\n",
    "\n",
    "- Most of the companies of cluster 3 stay in cluster 3.\n",
    "- Same for clusters 2 and 1.\n",
    "- The principal transition is from cluster 3 to 2.\n",
    "- Then it is from to to 3 and from 1 to 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "- Mettre des noms/pseudos des clusters\n",
    "- Changer la visualisation\n",
    "- Faire jointure avec toutes les ID\n",
    "- Conclure\n",
    "\n",
    "[2007, 2011[\n",
    "[2012, 2019] apres\n",
    "\n",
    "\n",
    "2007-2008 // 2011-2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentations on new rules of affectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .join(df_company_quant[['ENTITY_START_DATE', 'NB_EMPLOYEES_2018']],\n",
    "       on='id')\n",
    " .assign(AGE=lambda df: (2020 - df.ENTITY_START_DATE.dt.year))\n",
    " .drop(columns='ENTITY_START_DATE')\n",
    " .rename(columns={'NB_EMPLOYEES_2018': 'NB_EMPLOYEES'})\n",
    " .dropna()\n",
    " .assign(cluster=lambda df: gmm.predict(df))\n",
    " .cluster\n",
    " .unstack(0)\n",
    " .dropna()\n",
    " .assign(visited=lambda df: (df.loc[:, pd.date_range('2001', '2009', freq= 'y').year.astype(str)]\n",
    "                            .apply(set, axis=1)))\n",
    " .assign(test=lambda df: df['2009'].isin(df.visited))\n",
    " .assign(new_cluster=lambda df: df.apply(lambda df: df['2009'] not in df.visited, axis=1))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.date_range('2001', '2009', freq= 'y').year.astype(str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
